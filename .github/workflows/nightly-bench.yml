name: Nightly Benchmark

on:
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: read
  issues: write

jobs:
  nightly-benchmark:
    name: Run Nightly Performance Benchmark
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install requests

      - name: Build gateway image
        run: |
          docker build -f Dockerfile.gateway -t llm-gateway:nightly .

      - name: Start gateway container
        run: |
          docker run -d --name gateway \
            -p 8000:8000 \
            -e LOG_LEVEL=info \
            llm-gateway:nightly
          sleep 10

      - name: Wait for service to be ready
        timeout-minutes: 2
        run: |
          timeout 60 bash -c 'until curl -f http://localhost:8000/healthz; do echo "Waiting..."; sleep 2; done'
          echo "Service is ready!"

      - name: Warm-up requests
        run: |
          echo "Sending warm-up requests..."
          for i in {1..10}; do
            curl -X POST http://localhost:8000/generate \
              -H "Content-Type: application/json" \
              -d '{"prompt": "Warmup", "max_tokens": 50}' \
              --silent --output /dev/null
          done
          sleep 2

      - name: Download previous baseline
        id: download-baseline
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          name: benchmark-baseline
          path: bench/

      - name: Run comprehensive benchmark
        run: |
          echo "Running benchmark with 500 requests..."
          python bench/run_bench.py \
            --url http://localhost:8000 \
            -n 500 \
            -c 1 \
            -o bench/bench_results.json

      - name: Display benchmark results
        if: always()
        run: |
          echo "=== Benchmark Results ==="
          cat bench/bench_results.json | python -m json.tool

      - name: Compare against baseline
        id: compare
        if: steps.download-baseline.outcome == 'success'
        continue-on-error: true
        run: |
          echo "Comparing against baseline..."
          python bench/compare.py \
            --current bench/bench_results.json \
            --baseline bench/baseline.json

      - name: Set baseline if none exists
        if: steps.download-baseline.outcome != 'success'
        run: |
          echo "No baseline found. Setting current results as baseline."
          python bench/compare.py --set-baseline
          echo "baseline_created=true" >> $GITHUB_OUTPUT

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: nightly-benchmark-${{ github.run_number }}
          path: |
            bench/bench_results.json
          retention-days: 90

      - name: Upload baseline
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-baseline
          path: |
            bench/baseline.json
          retention-days: 365

      - name: Extract metrics for summary
        if: always()
        id: metrics
        run: |
          if [ -f bench/bench_results.json ]; then
            p50=$(jq -r '.latency_stats.p50_ms' bench/bench_results.json)
            p95=$(jq -r '.latency_stats.p95_ms' bench/bench_results.json)
            p99=$(jq -r '.latency_stats.p99_ms' bench/bench_results.json)
            throughput=$(jq -r '.throughput_rps' bench/bench_results.json)
            error_rate=$(jq -r '.error_rate' bench/bench_results.json)
            
            echo "p50=$p50" >> $GITHUB_OUTPUT
            echo "p95=$p95" >> $GITHUB_OUTPUT
            echo "p99=$p99" >> $GITHUB_OUTPUT
            echo "throughput=$throughput" >> $GITHUB_OUTPUT
            echo "error_rate=$error_rate" >> $GITHUB_OUTPUT
          fi

      - name: Generate performance summary
        if: always()
        run: |
          echo "## ðŸŒ™ Nightly Benchmark Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")" >> $GITHUB_STEP_SUMMARY
          echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Performance Metrics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| P50 Latency | ${{ steps.metrics.outputs.p50 }} ms |" >> $GITHUB_STEP_SUMMARY
          echo "| P95 Latency | ${{ steps.metrics.outputs.p95 }} ms |" >> $GITHUB_STEP_SUMMARY
          echo "| P99 Latency | ${{ steps.metrics.outputs.p99 }} ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Throughput | ${{ steps.metrics.outputs.throughput }} req/s |" >> $GITHUB_STEP_SUMMARY
          echo "| Error Rate | ${{ steps.metrics.outputs.error_rate }}% |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

      - name: Get container logs
        if: always()
        run: |
          echo "=== Gateway Container Logs ==="
          docker logs gateway --tail 100 || true

      - name: Stop containers
        if: always()
        run: |
          docker stop gateway || true
          docker rm gateway || true

      - name: Check regression status
        if: steps.compare.outcome == 'failure'
        run: |
          echo "## âš ï¸ Performance Regression Detected!" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "The nightly benchmark has detected a performance regression." >> $GITHUB_STEP_SUMMARY
          echo "Please review the benchmark results and investigate." >> $GITHUB_STEP_SUMMARY
          exit 1

  report-failure:
    name: Report Benchmark Failure
    runs-on: ubuntu-latest
    needs: nightly-benchmark
    if: failure()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Create issue on failure
        uses: actions/github-script@v7
        with:
          script: |
            const title = `ðŸ”´ Nightly Benchmark Failed - ${new Date().toISOString().split('T')[0]}`;
            const body = `
            ## Nightly Benchmark Failure Report
            
            The nightly performance benchmark has failed.
            
            **Details:**
            - **Run ID**: ${{ github.run_id }}
            - **Run Number**: ${{ github.run_number }}
            - **Commit**: ${{ github.sha }}
            - **Workflow**: ${{ github.workflow }}
            - **Date**: ${new Date().toUTCString()}
            
            **Possible causes:**
            - Performance regression detected (P95 > 20% or error rate > 1%)
            - Service startup failure
            - Infrastructure issues
            
            **Action Required:**
            1. Review the [workflow run logs](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
            2. Check the benchmark results artifact
            3. Compare against baseline metrics
            4. Investigate recent code changes
            
            ---
            *This issue was automatically created by the nightly benchmark workflow.*
            `;
            
            // Check if similar issue already exists
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'benchmark-failure'
            });
            
            const existingIssue = issues.data.find(issue => 
              issue.title.includes('Nightly Benchmark Failed')
            );
            
            if (existingIssue) {
              // Add comment to existing issue
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: existingIssue.number,
                body: `Another failure detected:\n\n${body}`
              });
              console.log(`Added comment to existing issue #${existingIssue.number}`);
            } else {
              // Create new issue
              const issue = await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: title,
                body: body,
                labels: ['benchmark-failure', 'performance', 'automated']
              });
              console.log(`Created new issue #${issue.data.number}`);
            }

  report-success:
    name: Report Benchmark Success
    runs-on: ubuntu-latest
    needs: nightly-benchmark
    if: success()
    steps:
      - name: Close related issues on success
        uses: actions/github-script@v7
        with:
          script: |
            // Find open benchmark failure issues
            const issues = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              state: 'open',
              labels: 'benchmark-failure'
            });
            
            for (const issue of issues.data) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issue.number,
                body: `âœ… Nightly benchmark passed successfully on ${new Date().toUTCString()}.\n\nClosing this issue as the regression appears to be resolved.\n\nRun: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`
              });
              
              await github.rest.issues.update({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issue.number,
                state: 'closed'
              });
              
              console.log(`Closed issue #${issue.number}`);
            }
